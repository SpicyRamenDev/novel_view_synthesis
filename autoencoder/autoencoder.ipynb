{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1117a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "107a62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c731a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "renders_path = Path('/mnt/ML/Datasets/shapenet renders/renders_pyrender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb076111",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(renders_path / 'metadatas.json', 'r') as f:\n",
    "    asset_metadatas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ce1688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_cache():\n",
    "    asset_metadatas = {}\n",
    "    for asset_id, asset in asset_renders_list.items():\n",
    "        with open(asset['metadata'], 'r') as f:\n",
    "            metadatas = json.load(f)\n",
    "        asset_metadatas[asset_id] = metadatas\n",
    "    with open('asset_metadatas_cache.json', 'w') as f:\n",
    "        json.dump(asset_metadatas, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d622c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87eacb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_asset(metadata, channel='rgba'):\n",
    "    return Image.open(renders_path / metadata['asset_id'] / metadata[f'{channel}_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b2007f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeBackground(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.normalize = transforms.Normalize([0.5], [0.5])\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        rgb = image[:3]\n",
    "        alpha = image[3]\n",
    "        norm_rgb = self.normalize(rgb)\n",
    "        return torch.mul(norm_rgb, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba53a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomHue(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def adjust_hue(self, image, hue):\n",
    "        rgb = image[:3]\n",
    "        alpha = image[3:]\n",
    "        rgb = transforms.functional.adjust_hue(rgb, hue)\n",
    "        image = torch.cat([rgb, alpha], 0)\n",
    "        return image\n",
    "    \n",
    "    def __call__(self, data, target):\n",
    "        hue = np.random.rand() - 0.5\n",
    "        data = self.adjust_hue(data, hue)\n",
    "        target = self.adjust_hue(target, hue)\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f588f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class AssetDataset(Dataset):\n",
    "    def __init__(self, data, data_transforms, data_augmentations=None):\n",
    "        self.data = data\n",
    "        self.data_transforms = data_transforms\n",
    "        self.data_augmentations = data_augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_image = load_asset(self.data[idx]['input'])\n",
    "        target_image = load_asset(self.data[idx]['target'])\n",
    "        input_image = transforms.ToTensor()(input_image)\n",
    "        target_image = transforms.ToTensor()(target_image)\n",
    "        if self.data_augmentations is not None:\n",
    "            input_image, target_image = self.data_augmentations(input_image, target_image)\n",
    "        input_image = self.data_transforms(input_image)\n",
    "        target_image = self.data_transforms(target_image)\n",
    "        return input_image, target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78c226b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(x):\n",
    "    invTrans = transforms.Compose([\n",
    "        transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                             std = [ 1/0.5 ]),\n",
    "        transforms.Normalize(mean = [ -0.5 ],\n",
    "                             std = [ 1., 1., 1. ]),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "    img = invTrans(x)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a61e572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(data, output, target):\n",
    "    fig = plt.figure(figsize=(2*len(data), 2*3))\n",
    "    for idx in np.arange(len(target)):\n",
    "        ax = fig.add_subplot(3, len(data), idx+1, xticks=[], yticks=[])\n",
    "        plt.imshow(get_image(data[idx]))\n",
    "        ax = fig.add_subplot(3, len(data), idx+1+len(data), xticks=[], yticks=[])\n",
    "        plt.imshow(get_image(output[idx]))\n",
    "        ax = fig.add_subplot(3, len(data), idx+1+len(data)*2, xticks=[], yticks=[])\n",
    "        plt.imshow(get_image(target[idx]))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "170a1993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessor(image_size):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        NormalizeBackground(image_size),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5a544e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def makeDataLoader(data, data_transforms, data_augmentations, seed=0, test_size=0.1, batch_size=16):\n",
    "    train_ids, valid_ids = train_test_split(data, test_size=test_size, random_state=seed)\n",
    "    train_dataset = AssetDataset(data, data_transforms, data_augmentations)\n",
    "    valid_dataset = AssetDataset(data, data_transforms)\n",
    "    \n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    \n",
    "    train_size = len(train_dataset)\n",
    "    valid_size = len(valid_dataset)\n",
    "    \n",
    "    return train_loader, valid_loader, train_size, valid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e37b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinearity = F.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "707a0a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_tokens, dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.size = dim\n",
    "        self.multiheadattention = nn.MultiheadAttention(num_tokens * (dim + 1), num_tokens)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        \n",
    "        onehots = F.one_hot(torch.arange(0, self.num_tokens)).repeat(xb.shape[0], 1)\n",
    "        tokens = xb.view(-1, num_tokens, dim)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58e56da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.size = size\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(256, 32, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*16*32, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 16*16*32),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (32, 16, 16)),\n",
    "            nn.Conv2d(32, 256, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 256, 2, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 2, stride=2),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 2, stride=2),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        output = xb\n",
    "        output = self.encoder(output)\n",
    "        output = self.middle(output)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a606b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_func, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_func(0.5*output+0.5, 0.5*target+0.5)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar('training loss',\n",
    "                          loss.data.item() / len(data),\n",
    "                          epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "        if batch_idx % 16 == 0:\n",
    "            writer.add_figure('predictions',\n",
    "                              plot_predictions(data[:4], output[:4], target[:4]),\n",
    "                              global_step=epoch * len(train_loader) + batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa9a2950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader, loss_func, epoch, metric=None):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            validation_loss += loss_func(0.5*output+0.5, 0.5*target+0.5).data.item()\n",
    "\n",
    "        validation_loss /= len(valid_loader.dataset)\n",
    "\n",
    "        writer.add_scalar('validation loss',\n",
    "                          validation_loss,\n",
    "                          epoch + 1)\n",
    "        \n",
    "        print(f'epoch {epoch}: Validation set: Average loss: {validation_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e74db68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, loss_func, lr=0.001, seed=0, epochs=30):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        train(model, train_loader, loss_func, optimizer, epoch)\n",
    "        validation(model, valid_loader, loss_func, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c63c8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "preprocess = get_preprocessor((image_size, image_size))\n",
    "data_transforms = preprocess\n",
    "data_augmentations = None#RandomHue()\n",
    "data = []\n",
    "for metadata in asset_metadatas:\n",
    "    input_data = next(filter(lambda m: m['name'] == 'angle_30', metadata))\n",
    "    target_data = next(filter(lambda m: m['name'] == 'angle_60', metadata))\n",
    "    data.append({'input': input_data, 'target': target_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b07156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "batch_size = 64\n",
    "train_loader, valid_loader, train_size, val_size = makeDataLoader(data, data_transforms, data_augmentations, seed=0, batch_size=batch_size)\n",
    "model = AutoEncoder(image_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9765d3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (middle): Sequential(\n",
       "    (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): Linear(in_features=8192, out_features=1000, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=1000, out_features=8192, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Unflatten(dim=1, unflattened_size=(32, 16, 16))\n",
       "    (9): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (16): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5c33b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_msssim import MS_SSIM, ms_ssim, SSIM, ssim\n",
    "\n",
    "class MS_SSIM_Loss(MS_SSIM):\n",
    "    def forward(self, img1, img2):\n",
    "        return  100*(1 - super(MS_SSIM_Loss, self).forward(img1, img2))\n",
    "    \n",
    "ms_ssim_loss = MS_SSIM_Loss(data_range=1.0, size_average=True, channel=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ff0632b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: Validation set: Average loss: 0.9337\n",
      "epoch 1: Validation set: Average loss: 0.7012\n",
      "epoch 2: Validation set: Average loss: 0.5679\n",
      "epoch 3: Validation set: Average loss: 0.4674\n",
      "epoch 4: Validation set: Average loss: 0.4387\n",
      "epoch 5: Validation set: Average loss: 0.4152\n",
      "epoch 6: Validation set: Average loss: 0.3981\n",
      "epoch 7: Validation set: Average loss: 0.5088\n",
      "epoch 8: Validation set: Average loss: 0.3613\n",
      "epoch 9: Validation set: Average loss: 0.5880\n",
      "epoch 10: Validation set: Average loss: 0.3502\n",
      "epoch 11: Validation set: Average loss: 0.3385\n",
      "epoch 12: Validation set: Average loss: 0.4410\n",
      "epoch 13: Validation set: Average loss: 0.3496\n",
      "epoch 14: Validation set: Average loss: 0.3372\n",
      "epoch 15: Validation set: Average loss: 0.3337\n",
      "epoch 16: Validation set: Average loss: 0.3478\n",
      "epoch 17: Validation set: Average loss: 0.3228\n",
      "epoch 18: Validation set: Average loss: 0.3219\n",
      "epoch 19: Validation set: Average loss: 0.3371\n",
      "epoch 20: Validation set: Average loss: 0.3167\n",
      "epoch 21: Validation set: Average loss: 0.3075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mms_ssim_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [20], line 4\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, loss_func, lr, seed, epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     validation(model, valid_loader, loss_func, epoch)\n",
      "Cell \u001b[0;32mIn [18], line 3\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, loss_func, optimizer, epoch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, train_loader, loss_func, optimizer, epoch):\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m use_cuda:\n\u001b[1;32m      5\u001b[0m             data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcuda(), target\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1357\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1357\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1360\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1313\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1312\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1313\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1314\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1315\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1161\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1161\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1164\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, valid_loader, ms_ssim_loss, seed=0, epochs=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
