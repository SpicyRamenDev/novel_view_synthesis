{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667b3f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc32b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8465f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/diffusion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5b50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from data.renders_data import load_metadatas, load_render, BackgroundColor, NormTorchToPil, RenderDataset, makeDataLoader, RandomHue, LatentDataset\n",
    "from diffusion.ddim import evaluate, train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5bf9777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessor(size):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        BackgroundColor((0.5, 0.5, 0.5)),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7065b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 32  # the generated image resolution\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # how many images to sample during evaluation\n",
    "    num_epochs = 20\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 1\n",
    "    save_image_batches = 50\n",
    "    save_model_epochs = 1\n",
    "    mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = 'ddim-cars-angle_30-res_128-2'  # the model namy locally and on the HF Hub\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    hub_private_repo = False  \n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da5e3362",
   "metadata": {},
   "outputs": [],
   "source": [
    "renders_path = Path('/mnt/ML/novel_view_synthesis/render/renders_20_views_all_classes_256')\n",
    "latents_path = Path('/mnt/ML/novel_view_synthesis/diffusion/latents')\n",
    "data_transforms = get_preprocessor((config.image_size, config.image_size))\n",
    "data_augmentations = None #RandomHue()\n",
    "data = load_metadatas(renders_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4822260",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metadatas in data:\n",
    "    for metadata in metadatas:\n",
    "        metadata['latent_path'] = str(latents_path / metadata['asset_id'] / f\"latent_{metadata['id']:05d}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e11ec197",
   "metadata": {},
   "outputs": [],
   "source": [
    "renders_path = Path('/mnt/ML/Datasets/shapenet renders/renders_old')\n",
    "cat_id = '02958343'\n",
    "cat_path = renders_path / cat_id\n",
    "asset_ids = [str(d.relative_to(renders_path)) for d in cat_path.iterdir()]\n",
    "data = [[{ 'path': renders_path / asset_id, 'rgba_path': 'color_angle30_res256x256.png' }] for asset_id in asset_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ef6c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained('vae-cars-360_res_64/checkpoints/checkpoint_0004.ckp')\n",
    "vae.enable_slicing()\n",
    "vae.requires_grad_(False)\n",
    "vae.cuda()\n",
    "_ = vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d239c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_latents(vae, metadatas_list, batch_size=16):\n",
    "    metadatas = [metadata for metadatas in metadatas_list for metadata in metadatas]\n",
    "    dataloader, size = makeDataLoader(metadatas, config, batch_size, RenderDataset, data_transforms, shuffle=False)\n",
    "    \n",
    "    for step, images in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        images = images.to(vae.device)\n",
    "        with torch.no_grad():\n",
    "            posterior = vae.encode(images).latent_dist\n",
    "            latents = posterior.mode()\n",
    "        for i, latent in enumerate(latents):\n",
    "            metadata = metadatas[step*batch_size+i]\n",
    "            Path(metadata['latent_path']).parent.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(latent, metadata['latent_path'])\n",
    "            \n",
    "def compute_std(metadatas_list, batch_size=16):\n",
    "    metadatas = [metadata for metadatas in metadatas_list for metadata in metadatas]\n",
    "    dataloader, size = makeDataLoader(metadatas, config, batch_size, LatentDataset, shuffle=True)\n",
    "    latents = next(iter(dataloader))\n",
    "    std = latents.std(unbiased=False)\n",
    "    return std\n",
    "    \n",
    "#save_latents(vae, data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee3be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# std = compute_std(data, batch_size=64)\n",
    "# latent = torch.load(data[7][0]['latent_path'])\n",
    "# print(latent.shape)\n",
    "# dec = vae.decode(latent.unsqueeze(0)).sample\n",
    "# print(latent.mean())\n",
    "# display(NormTorchToPil(latent[:3] / std / 2))\n",
    "# display(NormTorchToPil(dec[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac07a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data, train_data = data[:20], data[20:]\n",
    "train_data = [metadata for metadatas in train_data for metadata in metadatas]\n",
    "valid_data = [metadata for metadatas in valid_data for metadata in metadatas]\n",
    "# train_loader, train_size = makeDataLoader(train_data, config, config.train_batch_size, LatentDataset, None)\n",
    "# valid_loader, val_size = makeDataLoader(valid_data, config, config.train_batch_size, LatentDataset, None, shuffle=False)\n",
    "train_loader, train_size = makeDataLoader(train_data, config, config.train_batch_size, RenderDataset, data_transforms)\n",
    "valid_loader, val_size = makeDataLoader(valid_data, config, config.train_batch_size, RenderDataset, data_transforms, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "982fb0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "def load_model(path):\n",
    "    model = UNet2DModel.from_pretrained(path).cuda()\n",
    "    return model\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=3,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, ),  # the number of output channes for each UNet block\n",
    "    attention_head_dim=8,\n",
    "    down_block_types=( \n",
    "        \"DownBlock2D\", \n",
    "        \"AttnDownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "    ), \n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\", \n",
    "      ),\n",
    ")\n",
    "\n",
    "#model = load_model('ddim-cars-angle_60_res_128/unet')\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40c8d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler\n",
    "\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=1000, clip_sample=False, set_alpha_to_one=False, steps_offset=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7602ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bc3feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(train_size * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b935339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:: 100%|████████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:15<00:00,  2.86it/s]\n",
      "Training epoch 1:: 100%|████████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:12<00:00,  2.99it/s]\n",
      "Training epoch 2:: 100%|████████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:12<00:00,  3.01it/s]\n",
      "Training epoch 3:: 100%|████████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:13<00:00,  2.97it/s]\n",
      "Training epoch 4:: 100%|████████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:12<00:00,  2.98it/s]\n",
      "Training epoch 5:: 100%|████████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:13<00:00,  2.96it/s]\n",
      "Training epoch 6:: 100%|████████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:12<00:00,  2.98it/s]\n",
      "Training epoch 7:: 100%|████████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:13<00:00,  2.96it/s]\n",
      "Training epoch 8:: 100%|████████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:15<00:00,  2.89it/s]\n",
      "Training epoch 9:: 100%|████████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:14<00:00,  2.90it/s]\n",
      "Training epoch 10:: 100%|███████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:14<00:00,  2.91it/s]\n",
      "Training epoch 11:: 100%|███████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:14<00:00,  2.92it/s]\n",
      "Training epoch 12:: 100%|███████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:12<00:00,  2.98it/s]\n",
      "Training epoch 13:: 100%|███████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:14<00:00,  2.93it/s]\n",
      "Training epoch 14:: 100%|███████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:13<00:00,  2.94it/s]\n",
      "Training epoch 15:: 100%|███████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:12<00:00,  2.99it/s]\n",
      "Training epoch 16:: 100%|███████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:14<00:00,  2.89it/s]\n",
      "Training epoch 17:: 100%|███████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:14<00:00,  2.92it/s]\n",
      "Training epoch 18:: 100%|███████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:14<00:00,  2.92it/s]\n",
      "Training epoch 19:: 100%|███████████████████████████████████████████████████████████████████████████████████████| 217/217 [01:14<00:00,  2.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "from ldm import train_loop\n",
    "\n",
    "args = (config, model, noise_scheduler, optimizer, train_loader, lr_scheduler)#, vae, std)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eee77553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm import evaluate\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
    "evaluate(config, model, noise_scheduler, 100, 0, num_inference_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36a6ef1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mldm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process_image, repaint, sample_images\n\u001b[0;32m----> 3\u001b[0m image \u001b[38;5;241m=\u001b[39m process_image(Path(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m/\u001b[39m data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgba_path\u001b[39m\u001b[38;5;124m'\u001b[39m]), data_transforms)\n\u001b[1;32m      4\u001b[0m r \u001b[38;5;241m=\u001b[39m repaint(model, noise_scheduler, image, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      5\u001b[0m NormTorchToPil(r)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from ldm import process_image, repaint, sample_images\n",
    "\n",
    "image = process_image(Path(data[0]['path'] / data[0]['rgba_path']), data_transforms)\n",
    "r = repaint(model, noise_scheduler, image, 1.0, 1000)\n",
    "NormTorchToPil(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b54fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = valid_loader.dataset[0].cuda()\n",
    "image = image.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    z = vae.encode(image).latent_dist.mode()\n",
    "    rec = vae(image).sample\n",
    "display(NormTorchToPil(image[0][:1].repeat(3, 1, 1)))\n",
    "display(NormTorchToPil(-0.2*z[0][3:4].repeat(3, 1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7567c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm import sample_images\n",
    "\n",
    "images = sample_images(config, model, noise_scheduler, vae=vae, num_inference_steps=100)\n",
    "images = [NormTorchToPil(img) for img in images]\n",
    "for image in images:\n",
    "    display(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
